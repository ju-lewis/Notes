
## Normalisation vs Standardisation

Normalisation involves scaling based on the max and min values

Standardisation involves warping the scores to a normal distribution


## Data Entity Resolution

Is "A. Einstein", "Albert Einstein", and "Einstein" the same person?
Specialised language processing tools are needed to answer this.

## Missing Data

### Missing at random (MAR)
### Missing not at random (MNAR)
- Data is MNAR when the missing values on a variable are related to the values of that variable itself, even after controlling other variables
### Disguised Missing Data
Example:
	- Someone 'hiding' their birthday by putting 1st January 2024
This needs to be handled by assessing what values are *unusual*, using knowledge about the domain of the values.


#### Simple Imputation for Numerical Data:

**Imputation** - methods to guess missing data.
Statistical Measurement Imputation: Simple strategies to fill out numerical data records:
- Mean
- Median
- Mode

This does introduce bias into the data.


## Sampling from data
Sampling: Selecting a few examples to represent all the data
- Almost like reverse imputation

When to sample?
- When the dataset is too large to effectively process
### Sampling methods
- **Randomly sampling:** Choosing random records from the dataset with/without replacement
- **Stratified sampling:** You split the population into groups depending on the relevant features

Sampling challenges:
- Representative - has the same important properties as the parent population
- Balanced


## Data Capture

### REST APIs
REST (Representational State Transfer) APIs
A set of protocols (rules) that format the exchange of information between server and client.
Inside the data:

This follows typical HTTP request rules.
- Header
	- Accept:
	- Content-type:
- Body

An HTTP response takes:
- Header
- Content
- Status


### Crawling and Scraping

Web crawlers are known as *spiders*, *crawlers*, or *bots*

Synonym URLs are disregarded
Significant/dynamic pages must be visited more frequently

**Basic challenge**: there is no central index of the URLs you want


#### The Algorithm
Start by creating a data structure of pages to visit:
- Stack -> Depth first search
- Queue -> Breadth first search
- Priority Queue -> 'best' first search

1. Start with a URL
2. Extract all URLs from the parsed data from the start page
3. Add to data structure
4. Visit next page
5. Remove visited page

#### robots.txt
A file that notifies crawlers what to visit and what not to visit

What you **MUST** do:
- Only crawl allowed pages
- Respect `robots.txt`
- Be robust (immune to spider traps and malicious server behaviour)
- Be legal

What you **SHOULD** do:
- Be scalable
- Be efficient


### The Scraping Pipeline
1. Crawl the web for documents
2. Scrape info out of the documents
3. Clean and save the info


We can use the Python library `beautifulsoup`

Beautiful Soup is an HTML parsing library that makes data extraction easy:
```python
# Finding an element by ID:
soup.find(id="test-id")
```


## Important Info:
- When to use a spreadsheet vs relational DB
- Basic preprocessing techniques (scaling, imputation, sampling)
- REST APIs
- Website Crawling
- Scraping with Beautiful Soup
- Justifications for above techniques


